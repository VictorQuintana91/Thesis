{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation of Text into sentences\n",
    "Code from [Mastering Natural Langiage Processing with Python](https://www.packtpub.com/big-data-and-business-intelligence/mastering-natural-language-processing-python). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"Welcome readers. I hope you find it interesting. please fo reply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'please fo reply.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For large number of senteces try this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text='Hello everyone. Hope you are all fine and doing well. Hope that you will find the book interesting.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Hope you are all fine and doing well.',\n",
       " 'Hope that you will find the book interesting.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For splitting sentences into words\n",
    "This can be done using the `word_tokenize` as per below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=nltk.word_tokenize(\"PeirreVinken, 59 years old, will join as a nonexecutive director on Nov. 29.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PeirreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide some imput text:So, hello this is my first attempt to use python with nltk. Hope it goes well.\n"
     ]
    }
   ],
   "source": [
    "r=input(\"Please provide some imput text:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of this word is 19 words.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "print(\"The length of this word is\", len(word_tokenize(r)),\"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about other tokenizers\n",
    "There is the `TreeBankTokenizer` which tokenises according to Penn Treebank Corpus conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'nice', 'day.', 'I', 'hope', 'you', 'find', 'the', 'book', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\" Don't hesitate to ask questions\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about using regular expressions\n",
    "Regular expressions can be used to split text based on punctuation chars. Luckily, we won't need RegExs just yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer=WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\" Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you want to use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of instantiating a class, here is an alternative way to tokenize with regexs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('\\s+',gaps=True)\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about selecting words with a capital letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'She']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "print(capt.tokenize(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about using predefined regexs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' She secured 90.56 % in class X . She is a meritorious student']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "print(BlanklineTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation using a white_space tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X', '.', 'She', 'is', 'a', 'meritorious', 'student']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X . She is a meritorious student\"\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "print(WhitespaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split method can also be used to specify a white_space character or anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "['She', 'secured', '90.56', '%', 'in', 'class', 'X.', 'She', 'is', 'a', 'meritorious', 'student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student', '']\n"
     ]
    }
   ],
   "source": [
    "sent= \"She secured 90.56 % in class X. She is a meritorious student\"\n",
    "print(sent.split())     # Notice that these two...\n",
    "print(sent.split(' '))  # ...are equivalent\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(sent.split('\\n'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SpaceTokenizer` works in a very similar way to `sent.split(' ')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'She', 'secured', '90.56', '%', 'in', 'class', 'X', '\\n.', 'She', 'is', 'a', 'meritorious', 'student\\n']\n"
     ]
    }
   ],
   "source": [
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "print(SpaceTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if all we want it to tokenize words into lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' She secured 90.56 % in class X \\n. She is a meritorious student\\n']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n",
      "[' She secured 90.56 % in class X ', '. She is a meritorious student']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "print(BlanklineTokenizer().tokenize(sent))\n",
    "from nltk.tokenize import LineTokenizer\n",
    "print(LineTokenizer(blanklines='keep').tokenize(sent))\n",
    "print(LineTokenizer(blanklines='discard').tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the process of: \n",
    "1. eliminating punctuation, \n",
    "2. converting the entire text into lowercase or uppercase, \n",
    "3. changing numbers into words, \n",
    "4. expanding abbreviations, \n",
    "5. canonicalisation of text \n",
    "and so on.\n",
    "\n",
    "Let's start with punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review: \n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\t\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
