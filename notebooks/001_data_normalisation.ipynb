{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly \n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "print(cf.__version__)\n",
    "# Configure cufflings \n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# !pip install autocorrect\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For monitoring duration of pandas processes\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# To avoid RuntimeError: Set changed size during iteration\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
    "tqdm.pandas(desc=\"Progress:\")\n",
    "\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "# and `progress_map` instead of `map`\n",
    "# can also groupby:\n",
    "# df.groupby(0).progress_apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = getDF('/Users/falehalrashidi/Downloads/reviews_Books_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df[['reviewerID','asin','reviewText','helpful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Column for the denominator\n",
    "df2 = df1.assign(denom = df1['helpful'].progress_apply(lambda enum_denom:enum_denom[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Uniquekey Column\n",
    "df3 = df2.assign(uniqueKey = df['reviewerID'].str.cat(df['asin'].values.astype(str), sep='##'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns necessary for the normalisation\n",
    "df4 = df3[['uniqueKey', 'reviewText']]\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was necessary due to weird keyErrors that followed after trying to process the reviewText as a `pandas.DataFrame` and not as `pandas.Series`. After experimenting with both, I found that `pandas.Series.apply` is faster than `pandas.DataFrame.apply` and so I will hence work with `pandas.Series`. \n",
    "\n",
    "The assumption I require to make at this point before I follow is that `pandas` will not change the index of the reviews as those are being processed by my code and that in the end of my processing I will be able to re-associate those reviews with their **uniqueKey**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueKey_series_df = df4[['uniqueKey']]\n",
    "uniqueKey_series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(df4['reviewText'].progress_apply(lambda review: review.split(\"\\n\")[0]))\n",
    "reviews_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalisation\n",
    "<span style=\"color:red\">**NOTICE:** As I am in a hurry to complete the first pipeline, I will only work with the first 100k reviews.</span>\n",
    "\n",
    "* Tokenization <span style=\"color:blue\"> DONE </span>\n",
    "* Convert All Tokens to Lowercase <span style=\"color:blue\"> DONE </span>\n",
    "* Eliminate Punctuation <span style=\"color:blue\"> DONE </span>\n",
    "* Remove Stop Words <span style=\"color:blue\"> DONE </span>\n",
    "* Changing Numbers into Words <span style=\"color:blue\"> DONE </span>\n",
    "* Expand Abbreviations <span style=\"color:red\"> NOT AS EASY AS I THOUGHT AND DOES NOT ADD MUCH VALUE (Ask Stasha's opinion)</span> \n",
    "* Correct Spelling <span style=\"color:red\"> TOO SLOW (10h for 100k reviews)-->SO WONT DO</span>\n",
    "* Substituting Tokens with Synonyms <span style=\"color:green\"> TO DO</span>\n",
    "* Semantical Marking of Negatives <span style=\"color:blue\"> DONE (Ask Stasha's opinion) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_0_df = reviews_df['reviewText'][0:99999].progress_apply(lambda review: nltk.word_tokenize(review))\n",
    "step_0_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokens to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def convert_to_lowercase(review):\n",
    "\n",
    "    for i in range(len(review)):\n",
    "        review[i] = review[i].lower()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_df = step_0_df.progress_apply(lambda review: convert_to_lowercase(review))\n",
    "step_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def eliminate_punctuation(review, regex):\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "step_2_df = step_1_df.progress_apply(lambda review: eliminate_punctuation(review, regex))\n",
    "step_2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    return [token for token in review if not token in stop_words]\n",
    "\n",
    "step_3_df = step_2_df.progress_apply(lambda review: remove_stopwords(review))\n",
    "step_3_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Changing Numbers into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def numStringToWord(review, p):\n",
    "\n",
    "    for i in range(len(review)):\n",
    "        if(review[i].isdigit()):\n",
    "            review[i] = p.number_to_words(review[i])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_4_df = step_3_df.progress_apply(lambda review: numStringToWord(review, p))\n",
    "step_4_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from autocorrect import spell\n",
    "\n",
    "# def spellCheck(review):\n",
    "\n",
    "#     for i in range(len(review)):\n",
    "#         review[i] = spell(review[i])\n",
    "#     return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_5_df = step_4_df.progress_apply(lambda review: spellCheck(review))\n",
    "# step_5_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting Tokens with Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantical Marking of Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negation_tokenizer(review):\n",
    "\n",
    "    # regex to match negation tokens\n",
    "    negation_re = re.compile(\"\"\"(?x)(?:^(?:never|no|nothing|nowhere|noone|none|not|havent|\n",
    "            hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|\n",
    "            doesnt|didnt|isnt|arent|aint)$)|n't\"\"\")\n",
    "\n",
    "    alter_re = re.compile(\"\"\"(?x)(?:^(?:but|however|nevertheless|still|though|tho|yet)$)\"\"\")\n",
    "\n",
    "    neg_review_tokens = []\n",
    "    append_neg = False  # stores whether to add \"_NEG\"\n",
    "\n",
    "    for token in review:\n",
    "\n",
    "        # If append_neg is False\n",
    "        if append_neg == False:\n",
    "\n",
    "            # Check if the current token is a negation\n",
    "            if negation_re.match(token):\n",
    "                append_neg = True\n",
    "\n",
    "        # but if a negation has been previously identified, check if this is an  alteration\n",
    "        elif alter_re.match(token):\n",
    "            append_neg = False\n",
    "\n",
    "        # or if another negation appears\n",
    "        elif negation_re.match(token):\n",
    "            append_neg = False\n",
    "\n",
    "        # and if not then append the suffix\n",
    "        else:\n",
    "            token += \"_NEG\"\n",
    "\n",
    "        # append the new token in the return list\n",
    "        neg_review_tokens.append(token)\n",
    "\n",
    "    return neg_review_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_5_df = step_4_df.progress_apply(lambda review: negation_tokenizer(review))\n",
    "step_5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_5_df[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
