{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For monitoring duration of pandas processes\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "# To avoid RuntimeError: Set changed size during iteration\n",
    "tqdm.monitor_interval = 0\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\n",
    "tqdm.pandas(desc=\"Progress:\")\n",
    "\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "# and `progress_map` instead of `map`\n",
    "# can also groupby:\n",
    "# df.groupby(0).progress_apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df0 = pd.read_pickle('../data/interim/004_synonyms_grouped_1k.p')\n",
    "df0 = pd.read_pickle('../data/interim/002_keyed_nouns.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueKey</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT##000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN##000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3##000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65##000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV##000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueKey  \\\n",
       "0  A2XQ5LZHTD4AFT##000100039X   \n",
       "1   AF7CSSGV93RXN##000100039X   \n",
       "2  A1NPNGWBVD9AK3##000100039X   \n",
       "3  A3IS4WGMFR4X65##000100039X   \n",
       "4   AWLFVCT9128JV##000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_df00 = pd.read_pickle('../data/interim/003_dictionary.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_df00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book</td>\n",
       "      <td>1502803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>639620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>read</td>\n",
       "      <td>467228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>386404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>story</td>\n",
       "      <td>365799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  frequency\n",
       "0    book    1502803\n",
       "1     one     639620\n",
       "2    read     467228\n",
       "3    like     386404\n",
       "4   story     365799"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea\n",
    "Words that only appear once cannot be frequent words even in their own context; so they will be filtered out. Then lets calculate the average frequency for the remaining words--remember; this dictionary does not only concern nouns.\n",
    "\n",
    "<span style=\"color:red\"> Notice: grouping of noun synonyms done in `004_grouping_domain_synonyms` is repeated here once filtering out nouns is applied, since it will take far less time to be applied on the whole dataset once the latter is filter (`004_grouping_domain_synonyms` was aplied only on 1k reviews)  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.550540e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.394970e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.586737e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.100000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.502803e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "count  1.550540e+05\n",
       "mean   5.394970e+02\n",
       "std    6.586737e+03\n",
       "min    6.000000e+00\n",
       "25%    1.000000e+01\n",
       "50%    2.200000e+01\n",
       "75%    9.100000e+01\n",
       "max    1.502803e+06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00.loc[dictionary_df00['frequency'] > 5].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00['word'].loc[dictionary_df00['frequency'] > 4].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt4_dictionary_df01 = dictionary_df00.loc[dictionary_df00['frequency'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.722840e+05\n",
       "mean     4.860424e+02\n",
       "std      6.250750e+03\n",
       "min      5.000000e+00\n",
       "25%      8.000000e+00\n",
       "50%      1.800000e+01\n",
       "75%      7.400000e+01\n",
       "max      1.502803e+06\n",
       "Name: frequency, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_df00['frequency'].loc[dictionary_df00['frequency'] > 4].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39890"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use threshold for first quantile\n",
    "final_dic = gt4_dictionary_df01.loc[dictionary_df00['frequency'] < 8]\n",
    "len(final_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 39890/39890 [00:00<00:00, 1326705.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132394</th>\n",
       "      <td>wordlessness</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132395</th>\n",
       "      <td>ciasponsored</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132396</th>\n",
       "      <td>sophieannes</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132397</th>\n",
       "      <td>traster</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132398</th>\n",
       "      <td>tedlock</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  frequency  normalised\n",
       "132394   wordlessness          7    0.014403\n",
       "132395   ciasponsored          7    0.014403\n",
       "132396    sophieannes          7    0.014403\n",
       "132397        traster          7    0.014403\n",
       "132398        tedlock          7    0.014403"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dic_df01 = final_dic.assign(normalised = final_dic['frequency'].progress_apply(lambda frequency:frequency/486))\n",
    "final_dic_df01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin noun filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueKey</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT##000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN##000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3##000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65##000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV##000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    uniqueKey  \\\n",
       "0  A2XQ5LZHTD4AFT##000100039X   \n",
       "1   AF7CSSGV93RXN##000100039X   \n",
       "2  A1NPNGWBVD9AK3##000100039X   \n",
       "3  A3IS4WGMFR4X65##000100039X   \n",
       "4   AWLFVCT9128JV##000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin\n",
       "0  A2XQ5LZHTD4AFT  000100039X\n",
       "1   AF7CSSGV93RXN  000100039X\n",
       "2  A1NPNGWBVD9AK3  000100039X\n",
       "3  A3IS4WGMFR4X65  000100039X\n",
       "4   AWLFVCT9128JV  000100039X"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(df0.uniqueKey.str.split('##',1).tolist(),columns = ['userId','asin'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText\n",
       "0  [timeless,  gibran,  backs,  content,  means, ...\n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...\n",
       "2  [ first,  books,  recall,  collection,  gibran...\n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...\n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviewText = pd.DataFrame(df0['reviewText'])\n",
    "df_reviewText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin  \\\n",
       "0  A2XQ5LZHTD4AFT  000100039X   \n",
       "1   AF7CSSGV93RXN  000100039X   \n",
       "2  A1NPNGWBVD9AK3  000100039X   \n",
       "3  A3IS4WGMFR4X65  000100039X   \n",
       "4   AWLFVCT9128JV  000100039X   \n",
       "\n",
       "                                          reviewText  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...  \n",
       "2  [ first,  books,  recall,  collection,  gibran...  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.concat([df1, df_reviewText], axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:00<00:00, 1217178.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>wordCountBefore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userId        asin  \\\n",
       "0  A2XQ5LZHTD4AFT  000100039X   \n",
       "1   AF7CSSGV93RXN  000100039X   \n",
       "2  A1NPNGWBVD9AK3  000100039X   \n",
       "3  A3IS4WGMFR4X65  000100039X   \n",
       "4   AWLFVCT9128JV  000100039X   \n",
       "\n",
       "                                          reviewText  wordCountBefore  \n",
       "0  [timeless,  gibran,  backs,  content,  means, ...               49  \n",
       "1  [ prophet,  kahlil,  gibran,  thirty,  years, ...               19  \n",
       "2  [ first,  books,  recall,  collection,  gibran...               76  \n",
       "3  [prophet,  kahlil,  work,  world,  million,  c...              142  \n",
       "4  [gibran,  khalil,  gibran,  born,  one thousan...               48  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_01 = df_new.assign(wordCountBefore = df_new['reviewText'].progress_apply(lambda review:len(review)))\n",
    "df_new_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 39890/39890 [00:00<00:00, 1211063.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132394</td>\n",
       "      <td>wordlessness</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132395</td>\n",
       "      <td>ciasponsored</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132396</td>\n",
       "      <td>sophieannes</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132397</td>\n",
       "      <td>traster</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132398</td>\n",
       "      <td>tedlock</td>\n",
       "      <td>7</td>\n",
       "      <td>0.014403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          word  frequency  normalised\n",
       "0  132394  wordlessness          7    0.014403\n",
       "1  132395  ciasponsored          7    0.014403\n",
       "2  132396   sophieannes          7    0.014403\n",
       "3  132397       traster          7    0.014403\n",
       "4  132398       tedlock          7    0.014403"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dic_df01['word'] = final_dic_df01['word'].progress_apply(lambda word: word.replace(\" \",\"\"))\n",
    "final_dic_df01 = final_dic_df01.reset_index()\n",
    "final_dic_df01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordlessness': 0,\n",
       " 'ciasponsored': 1,\n",
       " 'sophieannes': 2,\n",
       " 'traster': 3,\n",
       " 'tedlock': 4,\n",
       " 'pestiferous': 5,\n",
       " 'himselfas': 6,\n",
       " 'shigeko': 7,\n",
       " 'poe': 8,\n",
       " 'aureus': 9,\n",
       " 'easiertoread': 10,\n",
       " 'joyrides': 11,\n",
       " 'simmis': 12,\n",
       " '2014genres': 13,\n",
       " 'pigafetta': 14,\n",
       " 'wyss': 15,\n",
       " 'psychodelic': 16,\n",
       " 'schoool': 17,\n",
       " 'hjelms': 18,\n",
       " 'boadt': 19,\n",
       " 'savona': 20,\n",
       " 'bettany': 21,\n",
       " 'teached': 22,\n",
       " 'pageandahalf': 23,\n",
       " 'pinch': 24,\n",
       " 'policyby': 25,\n",
       " 'usagainstthem': 26,\n",
       " 'oompaloompas': 27,\n",
       " 'vitually': 28,\n",
       " 'buhle': 29,\n",
       " 'lims': 30,\n",
       " 'welltitled': 31,\n",
       " 'costcos': 32,\n",
       " 'rabbithole': 33,\n",
       " 'whalens': 34,\n",
       " 'infomration': 35,\n",
       " 'rizzolli': 36,\n",
       " 'laughingdog': 37,\n",
       " 'gloomies': 38,\n",
       " 'mugwort': 39,\n",
       " 'lovescenes': 40,\n",
       " 'throughit': 41,\n",
       " 'agress': 42,\n",
       " 'wellsubstantiated': 43,\n",
       " 'esbat': 44,\n",
       " 'sothat': 45,\n",
       " 'celierian': 46,\n",
       " 'harlequinjunkie': 47,\n",
       " 'wisconsinmadison': 48,\n",
       " 'mandatory': 49,\n",
       " 'rezzians': 50,\n",
       " 'sheks': 51,\n",
       " 'spearman': 52,\n",
       " 'latisha': 53,\n",
       " 'pssst': 54,\n",
       " 'meiss': 55,\n",
       " 'flutterings': 56,\n",
       " 'sympton': 57,\n",
       " 'ethniccleansing': 58,\n",
       " 'gaslights': 59,\n",
       " 'posner': 60,\n",
       " 'makea': 61,\n",
       " 'grovelled': 62,\n",
       " 'daetrin': 63,\n",
       " 'rehabilitative': 64,\n",
       " 'canyonlands': 65,\n",
       " 'varity': 66,\n",
       " 'orthogonian': 67,\n",
       " 'cordry': 68,\n",
       " 'cail': 69,\n",
       " 'parallell': 70,\n",
       " 'varigated': 71,\n",
       " 'ididnt': 72,\n",
       " 'kosmitoras': 73,\n",
       " 'judaism': 74,\n",
       " 'rokesmith': 75,\n",
       " 'redicks': 76,\n",
       " 'prosecco': 77,\n",
       " 'nbs': 78,\n",
       " 'knickerbocker': 79,\n",
       " 'rugar': 80,\n",
       " 'shiptoship': 81,\n",
       " 'bernadino': 82,\n",
       " 'angermanagement': 83,\n",
       " 'suskins': 84,\n",
       " 'nonparametric': 85,\n",
       " 'noirlike': 86,\n",
       " 'cardwell': 87,\n",
       " 'blindnesses': 88,\n",
       " 'smallworld': 89,\n",
       " 'phal': 90,\n",
       " 'wml': 91,\n",
       " 'ebbtide': 92,\n",
       " 'factthe': 93,\n",
       " 'thassas': 94,\n",
       " 'dekker': 95,\n",
       " 'selfenhancement': 96,\n",
       " 'redbeards': 97,\n",
       " 'sladens': 98,\n",
       " '5yr': 99,\n",
       " 'frazen': 100,\n",
       " 'hillock': 101,\n",
       " 'coldbloodedness': 102,\n",
       " 'mundt': 103,\n",
       " 'shgall': 104,\n",
       " '60lbs': 105,\n",
       " 'stelian': 106,\n",
       " 'nextand': 107,\n",
       " 'koester': 108,\n",
       " 'kayso': 109,\n",
       " 'frishman': 110,\n",
       " 'greengage': 111,\n",
       " 'gurkhas': 112,\n",
       " 'loking': 113,\n",
       " 'expertness': 114,\n",
       " 'lukan': 115,\n",
       " 'olena': 116,\n",
       " 'impishness': 117,\n",
       " 'burmark': 118,\n",
       " 'kohima': 119,\n",
       " 'shilled': 120,\n",
       " 'rives': 121,\n",
       " 'goodside': 122,\n",
       " 'deatils': 123,\n",
       " 'causal': 124,\n",
       " 'blowjob': 125,\n",
       " 'mindkiller': 126,\n",
       " 'brigerton': 127,\n",
       " 'waacbo': 128,\n",
       " 'doorstopsized': 129,\n",
       " 'efts': 130,\n",
       " 'alsoand': 131,\n",
       " 'caughleigh': 132,\n",
       " 'wondla': 133,\n",
       " 'esvs': 134,\n",
       " 'eisley': 135,\n",
       " 'hardright': 136,\n",
       " 'responsable': 137,\n",
       " 'macraes': 138,\n",
       " 'hkan': 139,\n",
       " 'sketchpad': 140,\n",
       " 'holocron': 141,\n",
       " 'gabrial': 142,\n",
       " 'foreigness': 143,\n",
       " 'leake': 144,\n",
       " 'crowbars': 145,\n",
       " 'ontheroad': 146,\n",
       " 'soweto': 147,\n",
       " 'messanic': 148,\n",
       " 'antiarmor': 149,\n",
       " 'hipocracy': 150,\n",
       " 'tradings': 151,\n",
       " 'korriban': 152,\n",
       " 'govenor': 153,\n",
       " 'welp': 154,\n",
       " 'laterborns': 155,\n",
       " 'belowi': 156,\n",
       " 'thirty-ninethousand': 157,\n",
       " 'addi': 158,\n",
       " 'discused': 159,\n",
       " 'bibiography': 160,\n",
       " 'unterwegers': 161,\n",
       " 'brendan': 162,\n",
       " 'soda': 163,\n",
       " 'menuges': 164,\n",
       " 'somewaht': 165,\n",
       " 'vina': 166,\n",
       " 'occassionaly': 167,\n",
       " 'moqtada': 168,\n",
       " 'andby': 169,\n",
       " '2011author': 170,\n",
       " 'silurian': 171,\n",
       " 'nightblooming': 172,\n",
       " 'schurchs': 173,\n",
       " 'isopropanol': 174,\n",
       " 'ohmaes': 175,\n",
       " '5writing': 176,\n",
       " 'vezin': 177,\n",
       " 'gazers': 178,\n",
       " 'allthemore': 179,\n",
       " 'shechem': 180,\n",
       " 'unrolled': 181,\n",
       " 'alysa': 182,\n",
       " 'kronnenberg': 183,\n",
       " 'conure': 184,\n",
       " 'taillight': 185,\n",
       " 'sealts': 186,\n",
       " 'abased': 187,\n",
       " 'bonewitz': 188,\n",
       " 'klawans': 189,\n",
       " 'religiouslike': 190,\n",
       " 'razorbacks': 191,\n",
       " 'ldlc': 192,\n",
       " 'adzuki': 193,\n",
       " 'nonstatisticians': 194,\n",
       " 'selfstimulation': 195,\n",
       " 'greenstreet': 196,\n",
       " 'hiebert': 197,\n",
       " 'razia': 198,\n",
       " '5quart': 199,\n",
       " '150m': 200,\n",
       " 'lydda': 201,\n",
       " 'psr': 202,\n",
       " 'troutman': 203,\n",
       " 'se7': 204,\n",
       " 'chanus': 205,\n",
       " 'softpedaling': 206,\n",
       " 'woodcourt': 207,\n",
       " 'belateche': 208,\n",
       " 'nivs': 209,\n",
       " 'storiesof': 210,\n",
       " 'westcentral': 211,\n",
       " 'corella': 212,\n",
       " 'benedikt': 213,\n",
       " 'icequeen': 214,\n",
       " 'ballons': 215,\n",
       " '6lbs': 216,\n",
       " 'columned': 217,\n",
       " 'palawan': 218,\n",
       " 'chathrand': 219,\n",
       " 'beginningtoend': 220,\n",
       " 'frangible': 221,\n",
       " 'amazin': 222,\n",
       " 'kuti': 223,\n",
       " 'pseudonymn': 224,\n",
       " 'tamest': 225,\n",
       " 'naga': 226,\n",
       " 'principlebased': 227,\n",
       " 'egalatarian': 228,\n",
       " 'itselfas': 229,\n",
       " 'simcity': 230,\n",
       " 'dalet': 231,\n",
       " 'sawn': 232,\n",
       " 'jewishgentile': 233,\n",
       " 'tcol': 234,\n",
       " 'pullouts': 235,\n",
       " 'knobby': 236,\n",
       " 'americanenglish': 237,\n",
       " 'stuffand': 238,\n",
       " 'twixt': 239,\n",
       " 'dianne': 240,\n",
       " 'beppe': 241,\n",
       " 'refusenik': 242,\n",
       " 'recommneded': 243,\n",
       " 'faud': 244,\n",
       " 'cockfight': 245,\n",
       " 'circulator': 246,\n",
       " 'sisinlaw': 247,\n",
       " 'madsons': 248,\n",
       " 'cookbookthe': 249,\n",
       " 'jaywalkers': 250,\n",
       " 'schaum': 251,\n",
       " '11the': 252,\n",
       " 'sport': 253,\n",
       " 'bibliograpy': 254,\n",
       " 'nearreligious': 255,\n",
       " 'impossibles': 256,\n",
       " 'voyeuristically': 257,\n",
       " 'suspsense': 258,\n",
       " 'repairmen': 259,\n",
       " 'nonnuclear': 260,\n",
       " 'phonographic': 261,\n",
       " 'fallaciously': 262,\n",
       " 'lebows': 263,\n",
       " 'behavour': 264,\n",
       " 'frentis': 265,\n",
       " 'technologyand': 266,\n",
       " 'marxengels': 267,\n",
       " 'tryto': 268,\n",
       " 'warriorprophet': 269,\n",
       " 'restauranteurs': 270,\n",
       " 'colan': 271,\n",
       " 'ellena': 272,\n",
       " 'authencity': 273,\n",
       " 'okbomb': 274,\n",
       " 'aldi': 275,\n",
       " 'christinaity': 276,\n",
       " 'thatthese': 277,\n",
       " 'clemson': 278,\n",
       " 'chukrow': 279,\n",
       " 'perspicaciously': 280,\n",
       " 'fpws': 281,\n",
       " 'endeavouring': 282,\n",
       " 'argonos': 283,\n",
       " 'schutzs': 284,\n",
       " 'posssible': 285,\n",
       " 'p30': 286,\n",
       " 'weare': 287,\n",
       " 'countercultures': 288,\n",
       " 'emaleth': 289,\n",
       " 'multihued': 290,\n",
       " 'joshing': 291,\n",
       " 'druss': 292,\n",
       " 'hada': 293,\n",
       " 'largest': 294,\n",
       " 'chroot': 295,\n",
       " 'suited': 296,\n",
       " 'crutchfield': 297,\n",
       " 'ballantynes': 298,\n",
       " 'brastemp': 299,\n",
       " 'huri': 300,\n",
       " 'parfords': 301,\n",
       " 'rasenberger': 302,\n",
       " 'kore': 303,\n",
       " 'binjamin': 304,\n",
       " 'oaklands': 305,\n",
       " 'mitchellcoauthor': 306,\n",
       " 'starsseries': 307,\n",
       " 'buggered': 308,\n",
       " 'bflat': 309,\n",
       " 'quoteunquote': 310,\n",
       " 'nilus': 311,\n",
       " 'charwoman': 312,\n",
       " 'ede': 313,\n",
       " 'weismann': 314,\n",
       " 'tailgate': 315,\n",
       " 'life4': 316,\n",
       " 'wildcraft': 317,\n",
       " 'handstands': 318,\n",
       " 'speckles': 319,\n",
       " 'dude': 320,\n",
       " 'zelana': 321,\n",
       " 'twoplus': 322,\n",
       " 'sage': 323,\n",
       " 'cowled': 324,\n",
       " 'influentials': 325,\n",
       " 'modulus': 326,\n",
       " 'friendships': 327,\n",
       " 'englishisbn': 328,\n",
       " 'utilized': 329,\n",
       " 'sabans': 330,\n",
       " 'intersting': 331,\n",
       " 'pappa': 332,\n",
       " 'meaningless': 333,\n",
       " 'coochie': 334,\n",
       " 'hyperlinking': 335,\n",
       " 'sthe': 336,\n",
       " 'hightraffic': 337,\n",
       " 'youof': 338,\n",
       " 'egoself': 339,\n",
       " 'superglue': 340,\n",
       " 'carra': 341,\n",
       " 'lefthemisphere': 342,\n",
       " 'korkis': 343,\n",
       " 'bears': 344,\n",
       " 'asplundh': 345,\n",
       " 'hanagarne': 346,\n",
       " 'aj': 347,\n",
       " 'nonaristocratic': 348,\n",
       " 'herselfshe': 349,\n",
       " 'humilated': 350,\n",
       " 'adriel': 351,\n",
       " 'byatts': 352,\n",
       " 'lexicographical': 353,\n",
       " 'vulcano': 354,\n",
       " 'allnot': 355,\n",
       " 'beginningthe': 356,\n",
       " 'bloodmages': 357,\n",
       " 'prinicples': 358,\n",
       " 'hayness': 359,\n",
       " 'rubegoldberg': 360,\n",
       " 'stormchasers': 361,\n",
       " 'lousia': 362,\n",
       " 'madrien': 363,\n",
       " 'overregulated': 364,\n",
       " 'variola': 365,\n",
       " 'masklin': 366,\n",
       " 'brownmillers': 367,\n",
       " 'wayof': 368,\n",
       " 'fimiliar': 369,\n",
       " 'refractor': 370,\n",
       " 'selectionism': 371,\n",
       " 'reluctance': 372,\n",
       " 'vanishings': 373,\n",
       " 'raelians': 374,\n",
       " 'peachiness': 375,\n",
       " 'reisss': 376,\n",
       " 'ilona': 377,\n",
       " 'vers': 378,\n",
       " 'abo': 379,\n",
       " 'superpeck': 380,\n",
       " 'biberkopf': 381,\n",
       " 'shintoism': 382,\n",
       " 'phyical': 383,\n",
       " 'tormod': 384,\n",
       " 'juvenalia': 385,\n",
       " 'portays': 386,\n",
       " 'caymens': 387,\n",
       " 'neapolitans': 388,\n",
       " 'lhop': 389,\n",
       " 'whiteskinned': 390,\n",
       " 'williston': 391,\n",
       " 'semifictionalized': 392,\n",
       " 'overreact': 393,\n",
       " 'dalit': 394,\n",
       " 'tob': 395,\n",
       " 'remarkability': 396,\n",
       " 'grubbs': 397,\n",
       " 'portholes': 398,\n",
       " 'greated': 399,\n",
       " 'thomasalexandre': 400,\n",
       " 'mutnojmet': 401,\n",
       " 'massmurderers': 402,\n",
       " 'brogues': 403,\n",
       " 'aghia': 404,\n",
       " 'herer': 405,\n",
       " 'astronomy': 406,\n",
       " 'pennywell': 407,\n",
       " 'amunition': 408,\n",
       " 'technosavvy': 409,\n",
       " 'killik': 410,\n",
       " 'forreal': 411,\n",
       " 'habitants': 412,\n",
       " 'vees': 413,\n",
       " 'eyeballed': 414,\n",
       " 'toryboy': 415,\n",
       " 'sofrep': 416,\n",
       " 'strikingly': 417,\n",
       " 'happyhappy': 418,\n",
       " 'banditos': 419,\n",
       " 'polisher': 420,\n",
       " 'mustafar': 421,\n",
       " '5pound': 422,\n",
       " 'tracies': 423,\n",
       " 'lobotomist': 424,\n",
       " 'joltingly': 425,\n",
       " 'buh': 426,\n",
       " 'orchestrations': 427,\n",
       " 'twoweapon': 428,\n",
       " 'muscovites': 429,\n",
       " 'teppics': 430,\n",
       " 'chronologically': 431,\n",
       " 'deduct': 432,\n",
       " 'undergo': 433,\n",
       " 'silverwood': 434,\n",
       " 'deatha': 435,\n",
       " 'antinausea': 436,\n",
       " 'peninsulas': 437,\n",
       " 'personell': 438,\n",
       " 'cunaxa': 439,\n",
       " 'nerf': 440,\n",
       " 'rothfusss': 441,\n",
       " 'leyers': 442,\n",
       " 'higginss': 443,\n",
       " 'holo': 444,\n",
       " 'hayley': 445,\n",
       " 'grazida': 446,\n",
       " 'lempriere': 447,\n",
       " '5gtotal': 448,\n",
       " 'haugen': 449,\n",
       " 'meed': 450,\n",
       " 'deleos': 451,\n",
       " 'wroughtiron': 452,\n",
       " 'agreeing': 453,\n",
       " 'duelists': 454,\n",
       " 'muscovite': 455,\n",
       " 'albertfor': 456,\n",
       " 'goateed': 457,\n",
       " 'agendae': 458,\n",
       " 'resourcing': 459,\n",
       " 'negrons': 460,\n",
       " 'scaleable': 461,\n",
       " 'packham': 462,\n",
       " 'grattan': 463,\n",
       " '5kg': 464,\n",
       " 'clayers': 465,\n",
       " 'pesty': 466,\n",
       " 'sixshooters': 467,\n",
       " 'abridgments': 468,\n",
       " 'sekigahara': 469,\n",
       " 'bloodynine': 470,\n",
       " 'thatwould': 471,\n",
       " 'glotka': 472,\n",
       " 'emboss': 473,\n",
       " 'doto': 474,\n",
       " 'erno': 475,\n",
       " 'quillers': 476,\n",
       " 'herbivoracious': 477,\n",
       " 'vonni': 478,\n",
       " 'landholding': 479,\n",
       " 'offloom': 480,\n",
       " 'marielle': 481,\n",
       " 'alumnae': 482,\n",
       " 'missionary': 483,\n",
       " 'bezetov': 484,\n",
       " 'chinamen': 485,\n",
       " 'gus': 486,\n",
       " 'andwith': 487,\n",
       " 'hansulrich': 488,\n",
       " 'somnolence': 489,\n",
       " 'chastizing': 490,\n",
       " 'hdx': 491,\n",
       " 'alainn': 492,\n",
       " 'mateguas': 493,\n",
       " 'microtension': 494,\n",
       " 'heyman': 495,\n",
       " 'odoherty': 496,\n",
       " 'brobdingnagian': 497,\n",
       " 'rellenos': 498,\n",
       " 'whitemans': 499,\n",
       " 'contast': 500,\n",
       " 'hetchins': 501,\n",
       " 'geagley': 502,\n",
       " 'amazoners': 503,\n",
       " 'servant': 504,\n",
       " 'industriously': 505,\n",
       " 'sto': 506,\n",
       " 'gadflies': 507,\n",
       " 'brynley': 508,\n",
       " 'delica': 509,\n",
       " 'pinol': 510,\n",
       " 'alexandermoegerle': 511,\n",
       " 'togetherit': 512,\n",
       " 'smattered': 513,\n",
       " 'britishism': 514,\n",
       " 'pimm': 515,\n",
       " 'tranquilized': 516,\n",
       " 'motorways': 517,\n",
       " 'andau': 518,\n",
       " 'geserit': 519,\n",
       " 'soult': 520,\n",
       " 'chiefofstaff': 521,\n",
       " 'expanation': 522,\n",
       " 'boop': 523,\n",
       " 'productplacement': 524,\n",
       " 'tschichold': 525,\n",
       " 'pretoria': 526,\n",
       " 'inservice': 527,\n",
       " 'fiddlehead': 528,\n",
       " 'advancement': 529,\n",
       " 'pffft': 530,\n",
       " 'abridges': 531,\n",
       " 'dorning': 532,\n",
       " 'sugarville': 533,\n",
       " 'jemiah': 534,\n",
       " 'suppurating': 535,\n",
       " 'manipulatives': 536,\n",
       " 'phonicsbased': 537,\n",
       " 'responsiblefor': 538,\n",
       " 'worksit': 539,\n",
       " 'semiacademic': 540,\n",
       " 'merope': 541,\n",
       " 'expence': 542,\n",
       " 'oharrow': 543,\n",
       " 'kinselection': 544,\n",
       " 'procedings': 545,\n",
       " 'practive': 546,\n",
       " 'brainman': 547,\n",
       " 'vascillates': 548,\n",
       " 'tacker': 549,\n",
       " 'suborbital': 550,\n",
       " 'waaaayyy': 551,\n",
       " 'reguardless': 552,\n",
       " 'kharijites': 553,\n",
       " 'succcess': 554,\n",
       " 'eggregious': 555,\n",
       " 'p212': 556,\n",
       " 'spellbindingly': 557,\n",
       " 'pachita': 558,\n",
       " 'slainte': 559,\n",
       " 'queenthe': 560,\n",
       " 'samhita': 561,\n",
       " 'noufs': 562,\n",
       " 'icewater': 563,\n",
       " 'descripton': 564,\n",
       " 'posthippie': 565,\n",
       " 'jhs': 566,\n",
       " 'inscape': 567,\n",
       " 'marchese': 568,\n",
       " 'p139': 569,\n",
       " 'cunnane': 570,\n",
       " 'stigmatised': 571,\n",
       " 'usmle': 572,\n",
       " 'pajaritas': 573,\n",
       " 'biogas': 574,\n",
       " 'greedo': 575,\n",
       " 'stamos': 576,\n",
       " 'shirkers': 577,\n",
       " 'decontamination': 578,\n",
       " 'fugui': 579,\n",
       " 'pograms': 580,\n",
       " 'uncapable': 581,\n",
       " 'halfwritten': 582,\n",
       " 'werecoyote': 583,\n",
       " 'esotericists': 584,\n",
       " 'wouldlike': 585,\n",
       " 'thecase': 586,\n",
       " 'unflavored': 587,\n",
       " 'feiges': 588,\n",
       " 'diffee': 589,\n",
       " 'prinzel': 590,\n",
       " 'enoyable': 591,\n",
       " 'paser': 592,\n",
       " 'buruus': 593,\n",
       " 'sah': 594,\n",
       " 'fastaction': 595,\n",
       " 'americn': 596,\n",
       " 'webmage': 597,\n",
       " 'sluething': 598,\n",
       " 'alternadad': 599,\n",
       " 'glamourized': 600,\n",
       " 'growthmindset': 601,\n",
       " 'manxs': 602,\n",
       " 'telecommute': 603,\n",
       " '10mg': 604,\n",
       " 'lowerpaid': 605,\n",
       " 'labratory': 606,\n",
       " 'wetzel': 607,\n",
       " 'oversentimentality': 608,\n",
       " 'alyce': 609,\n",
       " 'seethat': 610,\n",
       " 'americansand': 611,\n",
       " 'p27': 612,\n",
       " 'narrows': 613,\n",
       " 'rashleigh': 614,\n",
       " 'hieber': 615,\n",
       " 'hangnails': 616,\n",
       " 'bogies': 617,\n",
       " 'unappealingly': 618,\n",
       " 'wristwatches': 619,\n",
       " 'levitated': 620,\n",
       " 'multiplexes': 621,\n",
       " 'edells': 622,\n",
       " 'outdated': 623,\n",
       " 'istria': 624,\n",
       " 'storyteller': 625,\n",
       " 'tariq': 626,\n",
       " 'metroplex': 627,\n",
       " 'dumba': 628,\n",
       " 'voldemorts': 629,\n",
       " 'litteraly': 630,\n",
       " 'giulias': 631,\n",
       " 'condensate': 632,\n",
       " 'ih': 633,\n",
       " 'wyndanos': 634,\n",
       " 'pagone': 635,\n",
       " 'lull': 636,\n",
       " 'applebee': 637,\n",
       " 'elyse': 638,\n",
       " 'fangtasia': 639,\n",
       " 'milleu': 640,\n",
       " 'protected': 641,\n",
       " 'kamon': 642,\n",
       " 'osama': 643,\n",
       " 'roid': 644,\n",
       " 'dyans': 645,\n",
       " 'bombmaker': 646,\n",
       " 'macintryes': 647,\n",
       " 'exlaw': 648,\n",
       " 'vulpine': 649,\n",
       " 'sata': 650,\n",
       " 'korelitzs': 651,\n",
       " 'toput': 652,\n",
       " 'flouridation': 653,\n",
       " '41s': 654,\n",
       " 'cants': 655,\n",
       " 'thiry': 656,\n",
       " 'impacted': 657,\n",
       " 'linus': 658,\n",
       " 'riverting': 659,\n",
       " 'junkscience': 660,\n",
       " 'mcelvaines': 661,\n",
       " 'copyandpaste': 662,\n",
       " 'hitchen': 663,\n",
       " 'unrra': 664,\n",
       " 'gbs': 665,\n",
       " 'auvers': 666,\n",
       " 'kak': 667,\n",
       " 'ql': 668,\n",
       " 'redmeat': 669,\n",
       " 'billiejo': 670,\n",
       " 'lacerates': 671,\n",
       " 'commitmentphobia': 672,\n",
       " 'organisers': 673,\n",
       " 'firststrike': 674,\n",
       " 'achivements': 675,\n",
       " 'lammle': 676,\n",
       " 'romanceaholic': 677,\n",
       " 'steeps': 678,\n",
       " 'labyrinth': 679,\n",
       " 'ischia': 680,\n",
       " 'myrers': 681,\n",
       " 'principe': 682,\n",
       " 'einmal': 683,\n",
       " 'fruitfly': 684,\n",
       " '91st': 685,\n",
       " 'clifford': 686,\n",
       " 'amberstones': 687,\n",
       " 'postimpressionist': 688,\n",
       " 'nearsuicidal': 689,\n",
       " 'tigres': 690,\n",
       " 'saxby': 691,\n",
       " 'whiteonwhite': 692,\n",
       " 'mankilling': 693,\n",
       " 'kammer': 694,\n",
       " 'rashelle': 695,\n",
       " 'rebbetzin': 696,\n",
       " 'considerd': 697,\n",
       " 'faithwhich': 698,\n",
       " 'tnh': 699,\n",
       " 'everyoneeven': 700,\n",
       " 'joonas': 701,\n",
       " 'souads': 702,\n",
       " 'rockerfeller': 703,\n",
       " 'prepolitical': 704,\n",
       " 'heggan': 705,\n",
       " 'koreanstyle': 706,\n",
       " 'fastapproaching': 707,\n",
       " 'mccain': 708,\n",
       " 'tsien': 709,\n",
       " 'fireroasted': 710,\n",
       " 'nasv': 711,\n",
       " 'cabinetlevel': 712,\n",
       " 'jerkier': 713,\n",
       " 'debtbased': 714,\n",
       " 'beancounter': 715,\n",
       " 'rundowns': 716,\n",
       " 'paolas': 717,\n",
       " 'lorrimer': 718,\n",
       " 'revoloution': 719,\n",
       " 'unclosed': 720,\n",
       " 'countrybycountry': 721,\n",
       " 'prechopped': 722,\n",
       " 'fitzjames': 723,\n",
       " 'misallocated': 724,\n",
       " 'endstate': 725,\n",
       " 'northen': 726,\n",
       " 'mindbeauty': 727,\n",
       " 'levar': 728,\n",
       " 'loveyou': 729,\n",
       " 'interventional': 730,\n",
       " 'gah': 731,\n",
       " 'initialized': 732,\n",
       " 'squints': 733,\n",
       " 'youget': 734,\n",
       " 'lgs': 735,\n",
       " 'utina': 736,\n",
       " 'corruptibility': 737,\n",
       " 'thigns': 738,\n",
       " 'biologys': 739,\n",
       " 'mafiastyle': 740,\n",
       " 'preview': 741,\n",
       " 'bait': 742,\n",
       " 'rabbis': 743,\n",
       " 'okinawas': 744,\n",
       " 'starbursts': 745,\n",
       " 'nissinen': 746,\n",
       " 'motility': 747,\n",
       " 'condemning': 748,\n",
       " 'shoaff': 749,\n",
       " 'ie5': 750,\n",
       " 'quasivictorian': 751,\n",
       " 'secunda': 752,\n",
       " 'largetype': 753,\n",
       " 'cons': 754,\n",
       " 'realscape': 755,\n",
       " 'annoted': 756,\n",
       " 'horserace': 757,\n",
       " 'obsessive': 758,\n",
       " 'mayaguez': 759,\n",
       " 'gilletteauthor': 760,\n",
       " 'linquistic': 761,\n",
       " 'sexaddict': 762,\n",
       " 'spanishstyle': 763,\n",
       " 'blixs': 764,\n",
       " 'litwin': 765,\n",
       " 'beeper': 766,\n",
       " 'moderation': 767,\n",
       " 'understandibly': 768,\n",
       " 'dragout': 769,\n",
       " 'bugialli': 770,\n",
       " '000ft': 771,\n",
       " 'susah': 772,\n",
       " 'psywar': 773,\n",
       " 'tepesh': 774,\n",
       " 'desley': 775,\n",
       " 'thenand': 776,\n",
       " 'graetz': 777,\n",
       " 'redacting': 778,\n",
       " 'osler': 779,\n",
       " 'inabilty': 780,\n",
       " 'p23': 781,\n",
       " 'buttonpushing': 782,\n",
       " 'finanacial': 783,\n",
       " 'kira': 784,\n",
       " 'medstar': 785,\n",
       " 'abouti': 786,\n",
       " 'preferrable': 787,\n",
       " 'ummayad': 788,\n",
       " 'tormance': 789,\n",
       " 'ashbrook': 790,\n",
       " 'bardell': 791,\n",
       " 'treelike': 792,\n",
       " 'lengthiness': 793,\n",
       " 'cybermen': 794,\n",
       " 'nambula': 795,\n",
       " 'molins': 796,\n",
       " 'indichova': 797,\n",
       " '86yearold': 798,\n",
       " 'waksal': 799,\n",
       " 'p185': 800,\n",
       " 'roerich': 801,\n",
       " 'martensons': 802,\n",
       " 'anthracite': 803,\n",
       " 'gallerstein': 804,\n",
       " 'mayham': 805,\n",
       " 'introducer': 806,\n",
       " 'businessworld': 807,\n",
       " 'poirot': 808,\n",
       " 'throe': 809,\n",
       " 'outofthebody': 810,\n",
       " 'midwife': 811,\n",
       " 'mj': 812,\n",
       " 'tayeb': 813,\n",
       " 'skyisfalling': 814,\n",
       " 'boxcutter': 815,\n",
       " 'mouselike': 816,\n",
       " 'antihistory': 817,\n",
       " 'tourvel': 818,\n",
       " 'ramban': 819,\n",
       " 'emplacement': 820,\n",
       " 'stewarts': 821,\n",
       " 'esto': 822,\n",
       " 'marco': 823,\n",
       " 'mengers': 824,\n",
       " 'dips': 825,\n",
       " 'tonios': 826,\n",
       " 'minervini': 827,\n",
       " 'alatristes': 828,\n",
       " 'spyplane': 829,\n",
       " 'anglophobia': 830,\n",
       " 'murel': 831,\n",
       " 'bodo': 832,\n",
       " 'readmore': 833,\n",
       " 'ual': 834,\n",
       " 'statecentric': 835,\n",
       " 'uncrossable': 836,\n",
       " 'greatpower': 837,\n",
       " 'totting': 838,\n",
       " 'unproveable': 839,\n",
       " 'chainofcommand': 840,\n",
       " 'allgrain': 841,\n",
       " 'committing': 842,\n",
       " 'appologize': 843,\n",
       " 'captious': 844,\n",
       " 'meddlers': 845,\n",
       " 'blueribbon': 846,\n",
       " 'problemas': 847,\n",
       " 'proletariats': 848,\n",
       " 'orner': 849,\n",
       " 'blabla': 850,\n",
       " 'negitive': 851,\n",
       " 'heirophant': 852,\n",
       " 'stross': 853,\n",
       " 'trammeled': 854,\n",
       " 'gringrich': 855,\n",
       " 'ninetyminute': 856,\n",
       " 'councillors': 857,\n",
       " 'foresighted': 858,\n",
       " 'mister': 859,\n",
       " 'biogaphy': 860,\n",
       " 'yds': 861,\n",
       " 'stength': 862,\n",
       " 'scourby': 863,\n",
       " 'barrymores': 864,\n",
       " 'scourbys': 865,\n",
       " 'hosps': 866,\n",
       " 'kossmann': 867,\n",
       " 'romanovich': 868,\n",
       " 'hermother': 869,\n",
       " 'identiy': 870,\n",
       " 'attendence': 871,\n",
       " 'foreknow': 872,\n",
       " 'tarpaper': 873,\n",
       " 'rawlinss': 874,\n",
       " 'rahabs': 875,\n",
       " 'everfaithful': 876,\n",
       " 'nonspecialized': 877,\n",
       " 'anthropomorphising': 878,\n",
       " 'leastunderstood': 879,\n",
       " 'environmental': 880,\n",
       " 'yn': 881,\n",
       " 'drennan': 882,\n",
       " 'sigfrid': 883,\n",
       " 'distinctly': 884,\n",
       " 'magritte': 885,\n",
       " 'clealy': 886,\n",
       " 'deeping': 887,\n",
       " 'lowfunctioning': 888,\n",
       " 'subby': 889,\n",
       " 'larken': 890,\n",
       " 'testators': 891,\n",
       " 'shite': 892,\n",
       " 'intentially': 893,\n",
       " 'abna': 894,\n",
       " 'birthpangs': 895,\n",
       " 'diseconomies': 896,\n",
       " 'overtechnical': 897,\n",
       " 'mannering': 898,\n",
       " 'construal': 899,\n",
       " 'nottoosubtle': 900,\n",
       " 'buccaneering': 901,\n",
       " 'maidservants': 902,\n",
       " 'pramal': 903,\n",
       " 'budhism': 904,\n",
       " 'prophets': 905,\n",
       " 'genada': 906,\n",
       " 'taked': 907,\n",
       " 'valise': 908,\n",
       " 'sportswear': 909,\n",
       " 'bascom': 910,\n",
       " 'cucullus': 911,\n",
       " 'hokitika': 912,\n",
       " 'tattooes': 913,\n",
       " 'immobilization': 914,\n",
       " 'tooltips': 915,\n",
       " 'denis': 916,\n",
       " 'posteritys': 917,\n",
       " 'faversham': 918,\n",
       " 'guma': 919,\n",
       " 'demise': 920,\n",
       " 'kirrick': 921,\n",
       " 'lowther': 922,\n",
       " 'dars': 923,\n",
       " 'mindlessly': 924,\n",
       " 'lincel': 925,\n",
       " 'gincy': 926,\n",
       " 'finneran': 927,\n",
       " 'dishonoured': 928,\n",
       " 'luddite': 929,\n",
       " 'sixgun': 930,\n",
       " 'kelman': 931,\n",
       " 'bootstrapped': 932,\n",
       " 'rustics': 933,\n",
       " 'beleaguer': 934,\n",
       " 'lollapalooza': 935,\n",
       " 'helos': 936,\n",
       " 'mayor': 937,\n",
       " 'turbotax': 938,\n",
       " 'robet': 939,\n",
       " 'kalush': 940,\n",
       " 'geta': 941,\n",
       " 'strayers': 942,\n",
       " 'similarlooking': 943,\n",
       " 'governmentprovided': 944,\n",
       " 'stettin': 945,\n",
       " 'vandenberg': 946,\n",
       " 'nightshirt': 947,\n",
       " 'grounded': 948,\n",
       " 'shuya': 949,\n",
       " 'robt': 950,\n",
       " 'pachakuti': 951,\n",
       " 'liliane': 952,\n",
       " 'indetail': 953,\n",
       " 'moocs': 954,\n",
       " 'netgallery': 955,\n",
       " 'infastructure': 956,\n",
       " 'heinleins': 957,\n",
       " 'reminiscenses': 958,\n",
       " 'specifially': 959,\n",
       " 'materialistically': 960,\n",
       " 'xylaras': 961,\n",
       " 'shinigami': 962,\n",
       " 'myrick': 963,\n",
       " 'barrytown': 964,\n",
       " 'astrogator': 965,\n",
       " 'tenniels': 966,\n",
       " 'machiavellians': 967,\n",
       " 'neurotoxin': 968,\n",
       " 'unpremeditated': 969,\n",
       " 'contrasty': 970,\n",
       " 'avedon': 971,\n",
       " 'yvonne': 972,\n",
       " 'maldivian': 973,\n",
       " 'kaname': 974,\n",
       " 'weeny': 975,\n",
       " 'coeditors': 976,\n",
       " 'drows': 977,\n",
       " 'mountains': 978,\n",
       " 'saltfree': 979,\n",
       " 'woodalls': 980,\n",
       " 'familyas': 981,\n",
       " 'mendenhall': 982,\n",
       " 'sixthgraders': 983,\n",
       " 'dtui': 984,\n",
       " 'protestation': 985,\n",
       " 'speedreaders': 986,\n",
       " 'pertwee': 987,\n",
       " 'leones': 988,\n",
       " 'trainwrecks': 989,\n",
       " 'revamps': 990,\n",
       " 'onesie': 991,\n",
       " 'lamottes': 992,\n",
       " 'insitute': 993,\n",
       " 'severns': 994,\n",
       " 'nonjudgmentalism': 995,\n",
       " 'percabeth': 996,\n",
       " 'blank': 997,\n",
       " 'briec': 998,\n",
       " 'erradicate': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dict = final_dic_df01['word'].to_dict()\n",
    "inv_filtered_dict = {v: k for k, v in filtered_dict.items()}\n",
    "inv_filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_words(review):\n",
    "    new_review = []\n",
    "    for word in review:\n",
    "        word = word.strip()\n",
    "        if word in inv_filtered_dict:\n",
    "            new_review.append(word)\n",
    "    return new_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:10<00:00, 57569.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df_new_02 = df_new_01.assign(filteredText = df_new_01['reviewText'].progress_apply(lambda review:filter_words(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 582711/582711 [00:00<00:00, 1287602.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>wordCountBefore</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>wordCountAfter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[timeless,  gibran,  backs,  content,  means, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>[messege, sermon, prophets, flows]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF7CSSGV93RXN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ prophet,  kahlil,  gibran,  thirty,  years, ...</td>\n",
       "      <td>19</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1NPNGWBVD9AK3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ first,  books,  recall,  collection,  gibran...</td>\n",
       "      <td>76</td>\n",
       "      <td>[catechism, texts, siddhartha, preachers, prop...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IS4WGMFR4X65</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  kahlil,  work,  world,  million,  c...</td>\n",
       "      <td>142</td>\n",
       "      <td>[claude, mastery, biographers]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWLFVCT9128JV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibran,  khalil,  gibran,  born,  one thousan...</td>\n",
       "      <td>48</td>\n",
       "      <td>[almustafa]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AFY0BT42DDYZV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[days,  gibrans,  gets,  literature,  yet,  bo...</td>\n",
       "      <td>177</td>\n",
       "      <td>[profits, twentysix, sage, metaphors]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A25P6DY6ARTCGZ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  gibran,  took,  millions,  encapsulate...</td>\n",
       "      <td>29</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1SP45I55GQIIE</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ words,  kahlil,  gibran,  divine,  wisdom,  ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[meanings]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A2E71VWXO59342</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  dispenses,  wisdom,  ones,  bids,  ...</td>\n",
       "      <td>29</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A2OP1HD9RGX5OW</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  myth,  work,  beauty,  whose,  every, ...</td>\n",
       "      <td>42</td>\n",
       "      <td>[simplicity, relies]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A2052JNVUPRTMT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ gets,  bedrock,  man,  prophet,  anyone,  wo...</td>\n",
       "      <td>43</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AGKPTMTR3UX1R</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[kahlil,  eighteen million,  poet,  mystic,  n...</td>\n",
       "      <td>50</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A1HS49P9TZRGV9</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ book,  collection,  remember,  around,  twel...</td>\n",
       "      <td>67</td>\n",
       "      <td>[default]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A2ZZHMT58ZMVCZ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[prophet,  years,  ship,  back,  homeland,  kn...</td>\n",
       "      <td>111</td>\n",
       "      <td>[departs, pillars, te, exile]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A3W43PSHRIG8KV</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ aware,  kahlil,  gibran,  read,  poem,  menu...</td>\n",
       "      <td>46</td>\n",
       "      <td>[menu]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A1TR1LU2JSZLUL</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[book,  gift,  journeyed,  overseas,  quest,  ...</td>\n",
       "      <td>39</td>\n",
       "      <td>[overseas]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ADIDQRLLR4KBQ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[atheist,  may,  seem,  strange,  people,  boo...</td>\n",
       "      <td>90</td>\n",
       "      <td>[phrases, metaphors, prophets]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A3AW2ZG0GP4SKN</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ book,  son,  stolen,  despair,  resonance,  ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[resonance]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2MMON52VMO7NT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[gibrans,  words,  lay,  bare,  simplicity,  t...</td>\n",
       "      <td>23</td>\n",
       "      <td>[simplicity, enrich, hail]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AR72Z89LACZ8Q</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>[ departure,  prophet,  people,  gather,  arou...</td>\n",
       "      <td>26</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId        asin  \\\n",
       "0   A2XQ5LZHTD4AFT  000100039X   \n",
       "1    AF7CSSGV93RXN  000100039X   \n",
       "2   A1NPNGWBVD9AK3  000100039X   \n",
       "3   A3IS4WGMFR4X65  000100039X   \n",
       "4    AWLFVCT9128JV  000100039X   \n",
       "5    AFY0BT42DDYZV  000100039X   \n",
       "6   A25P6DY6ARTCGZ  000100039X   \n",
       "7   A1SP45I55GQIIE  000100039X   \n",
       "8   A2E71VWXO59342  000100039X   \n",
       "9   A2OP1HD9RGX5OW  000100039X   \n",
       "10  A2052JNVUPRTMT  000100039X   \n",
       "11   AGKPTMTR3UX1R  000100039X   \n",
       "12  A1HS49P9TZRGV9  000100039X   \n",
       "13  A2ZZHMT58ZMVCZ  000100039X   \n",
       "14  A3W43PSHRIG8KV  000100039X   \n",
       "15  A1TR1LU2JSZLUL  000100039X   \n",
       "16   ADIDQRLLR4KBQ  000100039X   \n",
       "17  A3AW2ZG0GP4SKN  000100039X   \n",
       "18  A2MMON52VMO7NT  000100039X   \n",
       "19   AR72Z89LACZ8Q  000100039X   \n",
       "\n",
       "                                           reviewText  wordCountBefore  \\\n",
       "0   [timeless,  gibran,  backs,  content,  means, ...               49   \n",
       "1   [ prophet,  kahlil,  gibran,  thirty,  years, ...               19   \n",
       "2   [ first,  books,  recall,  collection,  gibran...               76   \n",
       "3   [prophet,  kahlil,  work,  world,  million,  c...              142   \n",
       "4   [gibran,  khalil,  gibran,  born,  one thousan...               48   \n",
       "5   [days,  gibrans,  gets,  literature,  yet,  bo...              177   \n",
       "6   [book,  gibran,  took,  millions,  encapsulate...               29   \n",
       "7   [ words,  kahlil,  gibran,  divine,  wisdom,  ...               35   \n",
       "8   [prophet,  dispenses,  wisdom,  ones,  bids,  ...               29   \n",
       "9   [book,  myth,  work,  beauty,  whose,  every, ...               42   \n",
       "10  [ gets,  bedrock,  man,  prophet,  anyone,  wo...               43   \n",
       "11  [kahlil,  eighteen million,  poet,  mystic,  n...               50   \n",
       "12  [ book,  collection,  remember,  around,  twel...               67   \n",
       "13  [prophet,  years,  ship,  back,  homeland,  kn...              111   \n",
       "14  [ aware,  kahlil,  gibran,  read,  poem,  menu...               46   \n",
       "15  [book,  gift,  journeyed,  overseas,  quest,  ...               39   \n",
       "16  [atheist,  may,  seem,  strange,  people,  boo...               90   \n",
       "17  [ book,  son,  stolen,  despair,  resonance,  ...               11   \n",
       "18  [gibrans,  words,  lay,  bare,  simplicity,  t...               23   \n",
       "19  [ departure,  prophet,  people,  gather,  arou...               26   \n",
       "\n",
       "                                         filteredText  wordCountAfter  \n",
       "0                  [messege, sermon, prophets, flows]               4  \n",
       "1                                                  []               0  \n",
       "2   [catechism, texts, siddhartha, preachers, prop...               8  \n",
       "3                      [claude, mastery, biographers]               3  \n",
       "4                                         [almustafa]               1  \n",
       "5               [profits, twentysix, sage, metaphors]               4  \n",
       "6                                                  []               0  \n",
       "7                                          [meanings]               1  \n",
       "8                                                  []               0  \n",
       "9                                [simplicity, relies]               2  \n",
       "10                                                 []               0  \n",
       "11                                                 []               0  \n",
       "12                                          [default]               1  \n",
       "13                      [departs, pillars, te, exile]               4  \n",
       "14                                             [menu]               1  \n",
       "15                                         [overseas]               1  \n",
       "16                     [phrases, metaphors, prophets]               3  \n",
       "17                                        [resonance]               1  \n",
       "18                         [simplicity, enrich, hail]               3  \n",
       "19                                                 []               0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_03 = df_new_02.assign(wordCountAfter = df_new_02['filteredText'].progress_apply(lambda review:len(review)))\n",
    "df_new_03[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining = 1 - df_new_03['wordCountAfter'].sum() / df_new_03['wordCountBefore'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noun reduction achieved:95.95373520483005%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average noun reduction achieved:\" + str(remaining*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules Mining Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|█████████▉| 59324/59325 [00:02<00:00, 27837.71it/s]\n",
      "Progress:: 100%|██████████| 59324/59324 [00:00<00:00, 1308829.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[[messege, sermon, prophets, flows], [], [cate...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[[periods, progresses, usage, thee, virtues, a...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[[], [continues, usfor, continues], [behavior]...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[[surgery, goodnight, claude, claude, sorts, t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[[], [], [fraser, fraser, perpetual, fraser, f...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                       filteredText  transactions\n",
       "0  000100039X  [[messege, sermon, prophets, flows], [], [cate...            30\n",
       "1  0002051850  [[periods, progresses, usage, thee, virtues, a...            31\n",
       "2  0002113570  [[], [continues, usfor, continues], [behavior]...             7\n",
       "3  0002117088  [[surgery, goodnight, claude, claude, sorts, t...             5\n",
       "4  000215725X  [[], [], [fraser, fraser, perpetual, fraser, f...            11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_bigReviews = pd.DataFrame(df_new_03[['asin','filteredText']].groupby(['asin'])['filteredText'].progress_apply(list))\n",
    "df_books_bigReviews = df_books_bigReviews.reset_index()\n",
    "df_books_bigReviews = df_books_bigReviews.assign(transactions = df_books_bigReviews['filteredText'].progress_apply(lambda reviews_lis:len(reviews_lis)))\n",
    "df_books_bigReviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# Support\n",
    "# Support is an indication of how frequently the itemset appears in the dataset.\n",
    "# Confidence\n",
    "# Confidence is an indication of how often the rule has been found to be true.\n",
    "# Lift\n",
    "# The ratio of the observed support to that expected if X and Y were independent.\n",
    "def apply_arm(transactions):\n",
    "    return list(apriori(transactions, min_support = 1/len(transactions), min_confidence = 1, min_lift = len(transactions), max_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 59324/59324 [5:25:02<00:00,  3.04it/s]     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>transactions</th>\n",
       "      <th>arm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[[messege, sermon, prophets, flows], [], [cate...</td>\n",
       "      <td>30</td>\n",
       "      <td>[((speaker, arabic), 0.03333333333333333, [Ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[[periods, progresses, usage, thee, virtues, a...</td>\n",
       "      <td>31</td>\n",
       "      <td>[((19yearolds, muck), 0.03225806451612903, [Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[[], [continues, usfor, continues], [behavior]...</td>\n",
       "      <td>7</td>\n",
       "      <td>[((homo, ancestors), 0.14285714285714285, [Ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[[surgery, goodnight, claude, claude, sorts, t...</td>\n",
       "      <td>5</td>\n",
       "      <td>[((goodnight, claude), 0.2, [OrderedStatistic(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[[], [], [fraser, fraser, perpetual, fraser, f...</td>\n",
       "      <td>11</td>\n",
       "      <td>[((17th, colony), 0.09090909090909091, [Ordere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                       filteredText  \\\n",
       "0  000100039X  [[messege, sermon, prophets, flows], [], [cate...   \n",
       "1  0002051850  [[periods, progresses, usage, thee, virtues, a...   \n",
       "2  0002113570  [[], [continues, usfor, continues], [behavior]...   \n",
       "3  0002117088  [[surgery, goodnight, claude, claude, sorts, t...   \n",
       "4  000215725X  [[], [], [fraser, fraser, perpetual, fraser, f...   \n",
       "\n",
       "   transactions                                                arm  \n",
       "0            30  [((speaker, arabic), 0.03333333333333333, [Ord...  \n",
       "1            31  [((19yearolds, muck), 0.03225806451612903, [Or...  \n",
       "2             7  [((homo, ancestors), 0.14285714285714285, [Ord...  \n",
       "3             5  [((goodnight, claude), 0.2, [OrderedStatistic(...  \n",
       "4            11  [((17th, colony), 0.09090909090909091, [Ordere...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_with_arm = df_books_bigReviews.assign(arm = df_books_bigReviews['filteredText'].progress_apply(lambda list_of_reviews:apply_arm(list_of_reviews)))\n",
    "books_with_arm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_important_nouns(arms):\n",
    "    imp_nns = []\n",
    "    if \"items\" in pd.DataFrame(arms).keys():\n",
    "        results = list(pd.DataFrame(arms)['items'])\n",
    "        for result in results:\n",
    "            if len(list(result)) > 4:\n",
    "                imp_nns = imp_nns + list(list(result))\n",
    "        if(len(imp_nns)==0):\n",
    "            for result in results:\n",
    "                if len(list(result)) > 3:\n",
    "                    imp_nns = imp_nns + list(list(result))            \n",
    "        return list(set(imp_nns))\n",
    "    return list(set(imp_nns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 59324/59324 [13:34:44<00:00,  1.21it/s]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>filteredText</th>\n",
       "      <th>transactions</th>\n",
       "      <th>arm</th>\n",
       "      <th>imp_nns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[[messege, sermon, prophets, flows], [], [cate...</td>\n",
       "      <td>30</td>\n",
       "      <td>[((speaker, arabic), 0.03333333333333333, [Ord...</td>\n",
       "      <td>[kneads, profits, preachers, territory, exile,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[[periods, progresses, usage, thee, virtues, a...</td>\n",
       "      <td>31</td>\n",
       "      <td>[((19yearolds, muck), 0.03225806451612903, [Or...</td>\n",
       "      <td>[declarations, towns, smaller, threatens, desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[[], [continues, usfor, continues], [behavior]...</td>\n",
       "      <td>7</td>\n",
       "      <td>[((homo, ancestors), 0.14285714285714285, [Ord...</td>\n",
       "      <td>[humane, homo, ancestors, michener]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[[surgery, goodnight, claude, claude, sorts, t...</td>\n",
       "      <td>5</td>\n",
       "      <td>[((goodnight, claude), 0.2, [OrderedStatistic(...</td>\n",
       "      <td>[surgery, sorts, goodnight, virtues, translato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[[], [], [fraser, fraser, perpetual, fraser, f...</td>\n",
       "      <td>11</td>\n",
       "      <td>[((17th, colony), 0.09090909090909091, [Ordere...</td>\n",
       "      <td>[treachery, fort, emperors, 17th, uk, mundane,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                       filteredText  \\\n",
       "0  000100039X  [[messege, sermon, prophets, flows], [], [cate...   \n",
       "1  0002051850  [[periods, progresses, usage, thee, virtues, a...   \n",
       "2  0002113570  [[], [continues, usfor, continues], [behavior]...   \n",
       "3  0002117088  [[surgery, goodnight, claude, claude, sorts, t...   \n",
       "4  000215725X  [[], [], [fraser, fraser, perpetual, fraser, f...   \n",
       "\n",
       "   transactions                                                arm  \\\n",
       "0            30  [((speaker, arabic), 0.03333333333333333, [Ord...   \n",
       "1            31  [((19yearolds, muck), 0.03225806451612903, [Or...   \n",
       "2             7  [((homo, ancestors), 0.14285714285714285, [Ord...   \n",
       "3             5  [((goodnight, claude), 0.2, [OrderedStatistic(...   \n",
       "4            11  [((17th, colony), 0.09090909090909091, [Ordere...   \n",
       "\n",
       "                                             imp_nns  \n",
       "0  [kneads, profits, preachers, territory, exile,...  \n",
       "1  [declarations, towns, smaller, threatens, desi...  \n",
       "2                [humane, homo, ancestors, michener]  \n",
       "3  [surgery, sorts, goodnight, virtues, translato...  \n",
       "4  [treachery, fort, emperors, 17th, uk, mundane,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_nns_df = books_with_arm.assign(imp_nns = books_with_arm['arm']\n",
    "                                   .progress_apply(lambda arms:get_important_nouns(arms)))\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>imp_nns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[kneads, profits, preachers, territory, exile,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[declarations, towns, smaller, threatens, desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[humane, homo, ancestors, michener]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[surgery, sorts, goodnight, virtues, translato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[treachery, fort, emperors, 17th, uk, mundane,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                            imp_nns\n",
       "0  000100039X  [kneads, profits, preachers, territory, exile,...\n",
       "1  0002051850  [declarations, towns, smaller, threatens, desi...\n",
       "2  0002113570                [humane, homo, ancestors, michener]\n",
       "3  0002117088  [surgery, sorts, goodnight, virtues, translato...\n",
       "4  000215725X  [treachery, fort, emperors, 17th, uk, mundane,..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_nns_df = imp_nns_df[['asin','imp_nns']]\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_nns_df.to_pickle(\"../data/interim/005_important_nouns.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:: 100%|██████████| 59324/59324 [00:00<00:00, 1183158.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>imp_nns</th>\n",
       "      <th>num_of_imp_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[kneads, profits, preachers, territory, exile,...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[declarations, towns, smaller, threatens, desi...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[humane, homo, ancestors, michener]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[surgery, sorts, goodnight, virtues, translato...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[treachery, fort, emperors, 17th, uk, mundane,...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                            imp_nns  \\\n",
       "0  000100039X  [kneads, profits, preachers, territory, exile,...   \n",
       "1  0002051850  [declarations, towns, smaller, threatens, desi...   \n",
       "2  0002113570                [humane, homo, ancestors, michener]   \n",
       "3  0002117088  [surgery, sorts, goodnight, virtues, translato...   \n",
       "4  000215725X  [treachery, fort, emperors, 17th, uk, mundane,...   \n",
       "\n",
       "   num_of_imp_nouns  \n",
       "0                26  \n",
       "1                73  \n",
       "2                 4  \n",
       "3                 7  \n",
       "4                39  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_nns_df = imp_nns_df.assign(num_of_imp_nouns = imp_nns_df['imp_nns'].progress_apply(lambda imp_nouns:len(imp_nouns)))\n",
    "imp_nns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1\n"
     ]
    }
   ],
   "source": [
    "import plotly \n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "print(cf.__version__)\n",
    "# Configure cufflings \n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out synonyms again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10385"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksWithNoImportantNouns = imp_nns_df.loc[imp_nns_df['num_of_imp_nouns'] == 0]\n",
    "len(booksWithNoImportantNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48939"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksWithNoImportantNouns = imp_nns_df.loc[imp_nns_df['num_of_imp_nouns'] != 0]\n",
    "len(booksWithNoImportantNouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>imp_nns</th>\n",
       "      <th>num_of_imp_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>[kneads, profits, preachers, territory, exile,...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>[declarations, towns, smaller, threatens, desi...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>[humane, homo, ancestors, michener]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002117088</td>\n",
       "      <td>[surgery, sorts, goodnight, virtues, translato...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>[treachery, fort, emperors, 17th, uk, mundane,...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0002219417</td>\n",
       "      <td>[humanlevel, smaller, conversion, periods, lic...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000222383X</td>\n",
       "      <td>[treasons, construct, expansion, captains, fav...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002226618</td>\n",
       "      <td>[coward, towering, territory, papers, macdonal...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000224053X</td>\n",
       "      <td>[fundamentalists, coast, pioneer, inconsistenc...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0002242052</td>\n",
       "      <td>[stretches, authorities, ludlum, drugdealers, ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002311216</td>\n",
       "      <td>[andersons, espionage, poirot, worldwide, open...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002550938</td>\n",
       "      <td>[fights, mysery, pollution, cusslers, threads,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000255383X</td>\n",
       "      <td>[ubi, shark, merge, beeper, phrases]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0002621509</td>\n",
       "      <td>[surgery, espionage, dolgun, paranoid, employe...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0002726874</td>\n",
       "      <td>[macdonalds, aboot, ye, smaller, theer, uk, al...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0002727463</td>\n",
       "      <td>[violets, armies, elizabethan, 17th, remarks, ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0004723724</td>\n",
       "      <td>[forth, profundis, masterpieces, shorter, soci...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000612609X</td>\n",
       "      <td>[homers, mastery, remarks, brighteyes, mirskya...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0006135129</td>\n",
       "      <td>[merchant, satisfies, port, ratings, atonement...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0006136389</td>\n",
       "      <td>[neanderthals, rat, authorities, remarks, phil...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                            imp_nns  \\\n",
       "0   000100039X  [kneads, profits, preachers, territory, exile,...   \n",
       "1   0002051850  [declarations, towns, smaller, threatens, desi...   \n",
       "2   0002113570                [humane, homo, ancestors, michener]   \n",
       "3   0002117088  [surgery, sorts, goodnight, virtues, translato...   \n",
       "4   000215725X  [treachery, fort, emperors, 17th, uk, mundane,...   \n",
       "5   0002219417  [humanlevel, smaller, conversion, periods, lic...   \n",
       "6   000222383X  [treasons, construct, expansion, captains, fav...   \n",
       "7   0002226618  [coward, towering, territory, papers, macdonal...   \n",
       "8   000224053X  [fundamentalists, coast, pioneer, inconsistenc...   \n",
       "9   0002242052  [stretches, authorities, ludlum, drugdealers, ...   \n",
       "10  0002311216  [andersons, espionage, poirot, worldwide, open...   \n",
       "11  0002550938  [fights, mysery, pollution, cusslers, threads,...   \n",
       "12  000255383X               [ubi, shark, merge, beeper, phrases]   \n",
       "13  0002621509  [surgery, espionage, dolgun, paranoid, employe...   \n",
       "14  0002726874  [macdonalds, aboot, ye, smaller, theer, uk, al...   \n",
       "15  0002727463  [violets, armies, elizabethan, 17th, remarks, ...   \n",
       "17  0004723724  [forth, profundis, masterpieces, shorter, soci...   \n",
       "18  000612609X  [homers, mastery, remarks, brighteyes, mirskya...   \n",
       "19  0006135129  [merchant, satisfies, port, ratings, atonement...   \n",
       "20  0006136389  [neanderthals, rat, authorities, remarks, phil...   \n",
       "\n",
       "    num_of_imp_nouns  \n",
       "0                 26  \n",
       "1                 73  \n",
       "2                  4  \n",
       "3                  7  \n",
       "4                 39  \n",
       "5                 32  \n",
       "6                 11  \n",
       "7                 23  \n",
       "8                 81  \n",
       "9                 14  \n",
       "10                16  \n",
       "11                 6  \n",
       "12                 5  \n",
       "13                 8  \n",
       "14                63  \n",
       "15                43  \n",
       "17                 6  \n",
       "18                38  \n",
       "19                15  \n",
       "20                40  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksWithNoImportantNouns[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/falehalrashidi/anaconda3/lib/python3.6/site-packages/plotly/plotly/plotly.py:224: UserWarning:\n",
      "\n",
      "Woah there! Look at all those points! Due to browser limitations, the Plotly SVG drawing functions have a hard time graphing more than 500k data points for line charts, or 40k points for other types of charts. Here are some suggestions:\n",
      "(1) Use the `plotly.graph_objs.Scattergl` trace object to generate a WebGl graph.\n",
      "(2) Trying using the image API to return an image instead of a graph URL\n",
      "(3) Use matplotlib\n",
      "(4) See if you can create your visualization with fewer data points\n",
      "\n",
      "If the visualization you're using aggregates points (e.g., box plot, histogram, etc.) you can disregard this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~falrashidi/105.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksWithNoImportantNouns['num_of_imp_nouns'].iplot(kind='histogram', bins=100, xTitle='Number of Important Nouns', yTitle='Number of Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
